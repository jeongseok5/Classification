{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZ5gyhjXsWVwN8392DgKUY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LJDhfdc7qKSb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicBlock(nn.Module): # for resent18 and resnet34\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super().__init__() # block을 생성하는데 사용할 거라서 super 안에 자신을 안넣어도 된다.\n",
        "\n",
        "        # residual function\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "        )\n",
        "        # shortcut\n",
        "        self.shortcut = nn.Sequential() # identity\n",
        "\n",
        "        # input/output channel dimension이 다르거나 stride != 1이면 feature map size가 작아지므로\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "\n",
        "\n",
        "class BottleNeck(nn.Module): # for resnet over 50 layers\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "\n",
        "resnet_settings = {\n",
        "    '18': [BasicBlock, [2, 2, 2, 2]],\n",
        "    '34': [BasicBlock, [3, 4, 6, 3]],\n",
        "    '50': [BottleNeck, [3, 4, 6, 3]],\n",
        "    '101': [BottleNeck, [3, 4, 23, 3]],\n",
        "    '152': [BottleNeck, [3, 8, 36, 3]]\n",
        "}\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, num_block, num_classes=2): # block: BasicBlock/BottleNeck\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.conv2 = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3 = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4 = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5 = self._make_layer(block, 512, num_block[3], 2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "\n",
        "\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1) # flatten하기 위해 reshape\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    }
  ]
}
